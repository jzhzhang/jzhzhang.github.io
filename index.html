<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <style type="text/css">
    @import url(https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300);
      /* Color scheme stolen from Sergey Karayev */
      a {
      /*color: #b60a1c;*/
      color: #1772d0;
      /*color: #bd0a36;*/
      text-decoration:none;
      }
      a:focus, a:hover {
      color: #f09228;
      text-decoration:none;
      }
      body,td,th,tr,p,a {
      font-family: 'Roboto', sans-serif;
      font-size: 15px;
      font-weight: 300;
      }
      strong {
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      /*font-family: 'Avenir Next';*/
      font-size: 15px;
      font-weight: 400;
      }
      heading {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
      font-size: 24px;
      font-weight: 400;
      }
      papertitle {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
      font-size: 15px;
      font-weight:500;
      }
      name {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      font-weight: 400;
      font-size: 32px;
      }
      .one
      {
      width: 160px;
      height: 140px;
      position: relative;
      }
      .two
      {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
      }
      .fade {
       transition: opacity .2s ease-in-out;
       -moz-transition: opacity .2s ease-in-out;
       -webkit-transition: opacity .2s ease-in-out;
      }
      span.highlight {
          background-color: #ffffd0;
      }
    </style>



  <title>Jiazhao Zhang</title>
  
  <meta name="author" content="Jiazhao Zhang | Âº†ÂòâÊõå">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
</head>




<!-- <style>
  pl {
    font-size: 18px;
  }
</style> -->


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jiazhao Zhang | Âº†ÂòâÊõå</name>
              </p>
              <p>I am a Ph.D. student at the <a  href="https://cfcs.pku.edu.cn/english/"> Center on Frontiers of Computing Studies </a> at Peking University, where I have been advised by Prof. <a href="https://hughw19.github.io/">He Wang</a> since 2022. Prior to this, I earned my M.S. degree from NUDT, under the supervision of Prof. <a href="http://kevinkaixu.net/index.html">Kai Xu</a>. I received my B.Eng. degree from Shandong University.
              </p>

              <p>
                My research goal is to develop intelligent and practical robots to enhance people's daily lives. My current research focuses on building intelligent navigation robots based on vision-language models. I am also interested in scene reconstruction and understanding.
              </p>
              
              

              <p style="text-align:center">
                <a href="mailto:zhngjizh@gmail.com">Email</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=zOZgpXwAAAAJ&hl=zh-CNJ">Google Scholar</a> &nbsp/&nbsp
                <a href="images/cv/jiazhao_Resume_2025.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://github.com/jzhzhang/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/new/jiazhao_zhang_image.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/new/jiazhao_zhang_image.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p> *: equal contribution; <sup>‚Ä†</sup>: corresponding author(s) </p>

            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="trackvla_stop()" onmouseover="trackvla_start()" >
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='trackvla_image'>
                <img src="images/trackvla/trackvla.gif" width="230" height="165">
                </div>
                <img src='images/trackvla/trackvla.png' width="230" height="165">
              </div>
              <script type="text/javascript">
                function trackvla_start() {
                  document.getElementById('trackvla_image').style.opacity = "1";
                }
                function trackvla_stop() {
                  document.getElementById('trackvla_image').style.opacity = "0";
                }
                trackvla_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://pku-epic.github.io/Uni-NaVid/">
              <papertitle>TrackVLA: Embodied Visual Tracking in the Wild</papertitle>
              </a>
              <br>
              <em>Shaoan Wang*, <b style="font-weight: bold;">Jiazhao Zhang*</b></em>, Minghan Li, Jiahang Liu
              , Anqi Li, Kui Wu, Fangwei Zhong, Junzhi Yu, Zhizheng Zhang<sup>‚Ä†</sup>      ,He Wang<sup>‚Ä†</sup>


              <br>
              <em><b style="font-weight: bold;">Arxiv Preprint</b></em>
              <br>
              <a href="https://arxiv.org/pdf/2505.23189">Paper</a>
              /
              <a href="https://github.com/wsakobe/TrackVLA">Code</a>
              /
              <a href="https://pku-epic.github.io/TrackVLA-web/">Project page</a>
              <p></p>
              <p>
                TrackVLA is a vision-language-action model capable of simultaneous object recognition and visual tracking, trained on a dataset of 1.7 million samples. It demonstrates robust tracking, long-horizon tracking, and cross-domain generalization across diverse challenging environments.
              </p>
            </td>
          </tr>



          <tr onmouseout="uni_navid_stop()" onmouseover="uni_navid_start()" >
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='uni_navid_image'>
                <img src="images/uni_navid/uni_navid.gif" width="230" height="165">
                </div>
                <img src='images/uni_navid/uni_navid.png' width="230" height="165">
              </div>
              <script type="text/javascript">
                function uni_navid_start() {
                  document.getElementById('uni_navid_image').style.opacity = "1";
                }
                function uni_navid_stop() {
                  document.getElementById('uni_navid_image').style.opacity = "0";
                }
                uni_navid_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://pku-epic.github.io/Uni-NaVid/">
              <papertitle>Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks</papertitle>
              </a>
              <br>
              <em><b style="font-weight: bold;">Jiazhao Zhang</b></em>,       Kunyu Wang      ,Shaoan Wang       ,Minghan Li      ,Haoran Liu
              ,Songlin Wei, Zhongyuan Wang    ,Zhizheng Zhang<sup>‚Ä†</sup>      ,He Wang<sup>‚Ä†</sup>


              <br>
              <em><b style="font-weight: bold;">RSS 2025</b></em>
              <br>
              <a href="https://arxiv.org/pdf/2412.06224">Paper</a>
              /
              <a href="https://github.com/jzhzhang/Uni-NaVid">Code</a>
              /
              <a href="https://pku-epic.github.io/Uni-NaVid/">Project page</a>
              <p></p>
              <p>
                We present Uni-NaVid, the first video-based vision-language-action (VLA) model designed to unify diverse embodied navigation tasks and enable seamless navigation for mixed long-horizon tasks in unseen real-world environments.
              </p>
            </td>
          </tr>


          <tr onmouseout="roboverse_stop()" onmouseover="roboverse_start()" >
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='roboverse_image'>
                <img src="images/roboverse/roboverse.gif" width="230" height="165">
                </div>
                <img src='images/roboverse/roboverse.jpg' width="230" height="165">
              </div>
              <script type="text/javascript">
                function roboverse_start() {
                  document.getElementById('roboverse_image').style.opacity = "1";
                }
                function roboverse_stop() {
                  document.getElementById('roboverse_image').style.opacity = "0";
                }
                roboverse_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://roboverseorg.github.io/">
              <papertitle>RoboVerse: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning</papertitle>
              </a>
              <br>
              Haoran Geng*, Feishi Wang*, Songlin Wei*, Yuyang Li*, Bangjun Wang*, Boshi An*, Charlie Tianyue Cheng*, Haozhe Lou, Peihao Li, Yen-Jen Wang, Yutong Liang, Dylan Goetting, Chaoyi Xu, Haozhe Chen, Yuxi Qian, Yiran Geng, Jiageng Mao, Weikang Wan, Mingtong Zhang, Jiangran Lyu, Siheng Zhao, <em><b style="font-weight: bold;">Jiazhao Zhang</b></em>, Jialiang Zhang, Chengyang Zhao, Haoran Lu, Yufei Ding, Ran Gong, Yuran Wang, Yuxuan Kuang, Ruihai Wu, Baoxiong Jia, Carlo Sferrazza, Hao Dong, Siyuan Huang, Koushil Sreenath, Yue Wang‚Ä†, Jitendra Malik‚Ä†, Pieter Abbeel‚Ä†
              <br>
              <em><b style="font-weight: bold;">RSS 2025</b></em>
              <br>
              <a href="https://roboverseorg.github.io/static/pdfs/roboverse.pdf">Paper</a>
              /
              <a href="https://github.com/RoboVerseOrg/RoboVerse">Code</a>
              /
              <a href="https://roboverseorg.github.io/">Project page</a>
              <p></p>
              <!-- <p>
                We present Uni-NaVid, the first video-based vision-language-action (VLA) model designed to unify diverse embodied navigation tasks and enable seamless navigation for mixed long-horizon tasks in unseen real-world environments.
              </p> -->
            </td>
          </tr>








          <tr onmouseout="navid_stop()" onmouseover="navid_start()" >
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='navid_image'>
                <img src="images/navid/navid.gif" width="230" height="165">
                </div>
                <img src='images/navid/navid.png' width="230" height="165">
              </div>
              <script type="text/javascript">
                function navid_start() {
                  document.getElementById('navid_image').style.opacity = "1";
                }
                function navid_stop() {
                  document.getElementById('navid_image').style.opacity = "0";
                }
                navid_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://pku-epic.github.io/NaVid/">
              <papertitle>NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation</papertitle>
              </a>
              <br>
              <em><b style="font-weight: bold;">Jiazhao Zhang</b></em>*,       Kunyu Wang*      ,Rongtao Xu*       ,Gengze Zhou       ,Yicong Hong
              ,Xiaomeng Fang      ,Qi Wu    ,Zhizheng Zhang<sup>‚Ä†</sup>      ,He Wang<sup>‚Ä†</sup>


              <br>
              <em><b style="font-weight: bold;">RSS 2024</b></em>
              <br>
              <a href="https://arxiv.org/abs/2402.15852.pdf">Paper</a>
              /
              <a href="https://github.com/jzhzhang/NaVid-VLN-CE">Code</a>
              /
              <a href="https://pku-epic.github.io/NaVid/">Project page</a>
              <p></p>
              <p>
                NaVid makes the first endeavour to showcase the capability of VLMs to achieve state-of-the-art level navigation performance without any maps, odometer and depth inputs. Following human instruction, NaVid only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the next-step action. 
              </p>
            </td>
          </tr>



          <tr onmouseout="open6DOR_stop()" onmouseover="open6DOR_start()" >
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='open6DOR_image'>
                <img src="images/open6DOR/open6DOR.gif" width="230" height="165">
                </div>
                <img src='images/open6DOR/open6DOR.jpg' width="230" height="165">
              </div>
              <script type="text/javascript">
                function open6DOR_start() {
                  document.getElementById('open6DOR_image').style.opacity = "1";
                }
                function open6DOR_stop() {
                  document.getElementById('open6DOR_image').style.opacity = "0";
                }
                open6DOR_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://pku-epic.github.io/Open6DOR/">
              <papertitle>Open6DOR: Benchmarking Open-instruction 6-DoF Object Rearrangement and A VLM-based Approach</papertitle>
              </a>
              <br>

              Yufei Ding*, Haoran Geng*, Chaoyi Xu, Xiaomeng Fang, <em><b style="font-weight: bold;">Jiazhao Zhang</b></em>, Songlin Wei, Qiyu Dai, Zhizheng Zhang, He Wang<sup>‚Ä†</sup>

              <br>
              <em><b style="font-weight: bold;">IROS 2024</b></em>
              <br>
              <a href="https://github.com/Selina2023/Open6DOR">Code</a>
              /
              <a href="https://pku-epic.github.io/Open6DOR/">Project page</a>
              <p></p>
              <p>
                We present Open6DOR, a challenging and comprehensive benchmark for open-instruction 6-DoF object rearrangement tasks. Following this, we propose a zero-shot method, Open6DORGPT, which achieves SOTA performance and proves effective in demanding simulation environments and real-world scenarios.
              </p>
            </td>
          </tr>


          <tr onmouseout="cvpr_mc_stop()" onmouseover="cvpr_mc_start()" >
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cvpr_mc_image'>
                <img src="images/cvpr_2024_mc/cvpr_mc.gif" width="230" height="165">
                </div>
                <img src='images/cvpr_2024_mc/cvpr_mc.png' width="230" height="165">
              </div>
              <script type="text/javascript">
                function cvpr_mc_start() {
                  document.getElementById('cvpr_mc_image').style.opacity = "1";
                }
                function cvpr_mc_stop() {
                  document.getElementById('cvpr_mc_image').style.opacity = "0";
                }
                cvpr_mc_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2401.07745.pdf">
              <papertitle>MaskClustering: View Consensus based Mask Graph Clustering for Open-Vocabulary 3D Instance Segmentation</papertitle>
              </a>
              <br>
              Mi Yan, <em><b style="font-weight: bold;">Jiazhao Zhang</b></em>, Yan Zhu, He Wang<sup>‚Ä†</sup>
              <br>
              <em><b style="font-weight: bold;">CVPR 2024</b></em>
              <br>
              <a href="https://arxiv.org/abs/2401.07745.pdf">Paper</a>
              /
              <a href="https://github.com/PKU-EPIC/MaskClustering">Code</a>
              /
              <!-- <a href="https://github.com/yjtang249/MIPSFusion">Code</a> -->
              <a href="https://pku-epic.github.io/MaskClustering/">Project page</a>
              <p></p>
              <p>
                We propose a robust zero-shot 3D instance segmentation method that leverages the 3D view consensus of 2D candidate masks. Our method can integrate with a 2D visual foundation model (e.g., CLIP) to achieve open-vocabulary 3D instance segmentation.
              </p>
            </td>
          </tr>



          <tr onmouseout="icra_gamma_stop()" onmouseover="icra_gamma_start()" >
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='icra_gamma_image'>
                <img src="images/icra_2024_gamma/icra_2024_video.gif" width="230" height="165">
                </div>
                <img src='images/icra_2024_gamma/icra_2024_img.png' width="230" height="165">
              </div>
              <script type="text/javascript">
                function icra_gamma_start() {
                  document.getElementById('icra_gamma_image').style.opacity = "1";
                }
                function icra_gamma_stop() {
                  document.getElementById('icra_gamma_image').style.opacity = "0";
                }
                icra_gamma_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://pku-epic.github.io/GAMMA/">
              <papertitle>GAMMA: Graspability-Aware Mobile MAnipulation
              Policy Learning based on Online Grasping Pose Fusion</papertitle>
              </a>
              <br>
              <em><b style="font-weight: bold;">Jiazhao Zhang</b></em>*, Nandiraju Gireesh*, Jilong Wang, Xiaomeng Fang, Chaoyi Xu,  Weiguang Chen,  Liu Dai, He Wang<sup>‚Ä†</sup>
              <br>
              <em><b style="font-weight: bold;">ICRA 2024</b></em>
              <br>
              <a href="https://arxiv.org/pdf/2309.15459.pdf">Paper</a>
              /
              <a href="https://github.com/user432/gamma">Code</a>
              /
              <!-- <a href="https://github.com/yjtang249/MIPSFusion">Code</a> -->
              <a href="https://pku-epic.github.io/GAMMA/">Project page</a>
              <p></p>
              <p>
                We propose a graspability-aware mobile manipulation approach powered by an online grasping pose fusion framework that enables a temporally consistent grasping observation.
              </p>
            </td>
          </tr>




          <tr onmouseout="siga_mips_stop()" onmouseover="siga_mips_start()" >
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='siga_mips_image'>
                <img src="images/siga_mips/siga_mips.gif" width="230" height="165">
                </div>
                <img src='images/siga_mips/siga_mips.png' width="230" height="165">
              </div>
              <script type="text/javascript">
                function siga_mips_start() {
                  document.getElementById('siga_mips_image').style.opacity = "1";
                }
                function siga_mips_stop() {
                  document.getElementById('siga_mips_image').style.opacity = "0";
                }
                siga_mips_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2308.08741.pdf">
              <papertitle>MIPS-Fusion: Multi-Implicit-Submaps for Scalable and Robust Online Neural RGB-D Reconstruction</papertitle>
              </a>
              <br>
              Yijie Tang*, <em><b style="font-weight: bold;">Jiazhao Zhang</b></em>*, Zhinan Yu, He Wang, Kai Xu<sup>‚Ä†</sup>
              <br>
              <em><b style="font-weight: bold;">ACM Transactions on Graphics (SIGGRAPH Asia 2023)</b></em>
              <br>
              <a href="https://arxiv.org/pdf/2308.08741.pdf">Paper</a>
              /
              <a href="https://github.com/yjtang249/MIPSFusion">Code</a>
              <p></p>
              <p>
                We introduce MIPS-Fusion, a robust and scalable online RGB-D reconstruction method based on a novel neural implicit representation ‚Äì multi-implicit-submap.
              </p>
            </td>
          </tr>




          <tr onmouseout="cvpr_3dobjectnav_stop()" onmouseover="cvpr_3dobjectnav_start()" >
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cvpr_3dobjectnav_image'>
                <img src="images/cvpr_3dobjectnav/3dobjectnav.gif" width="230" height="165">
                </div>
                <img src='images/cvpr_3dobjectnav/3dobjectnav.png' width="230" height="165">
              </div>
              <script type="text/javascript">
                function cvpr_3dobjectnav_start() {
                  document.getElementById('cvpr_3dobjectnav_image').style.opacity = "1";
                }

                function cvpr_3dobjectnav_stop() {
                  document.getElementById('cvpr_3dobjectnav_image').style.opacity = "0";
                }
                cvpr_3dobjectnav_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://github.com/jzhzhang/3DAwareNav">
              <papertitle>3D-Aware Object Goal Navigation via Simultaneous Exploration and Identification</papertitle>
              </a>
              <br>
              <em><b style="font-weight: bold;">Jiazhao Zhang</b></em>* , Liu Dai*, Fanpeng Meng, Qingnan Fan, Xuelin Chen, Kai Xu, He Wang<sup>‚Ä†</sup>
              <br>
              <em><b style="font-weight: bold;">CVPR 2023</b></em>
              <br>
              <!-- <a href="https://dreamfusion3d.github.io/">project page</a> -->
              <!-- / -->
              <a href="https://arxiv.org/pdf/2212.00338">Paper</a>
              /
              <a href="https://github.com/jzhzhang/3DAwareNav">Code</a>
              /
              <a href="https://pku-epic.github.io/3D-Aware-ObjectNav/">Project page</a>
              <p></p>
              <p>
                We propose a framework for the challenging 3D-aware ObjectNav based on two straightforward sub-policies, namely corner-guided exploration policy and category-aware identification policy.

              </p>
            </td>
          </tr>





          <tr onmouseout="icra_nerfgrasp_stop()" onmouseover="icra_nerfgrasp_start()" >
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='icra_nerfgrasp_image'>
                <img src="images/icra_nerfgrasp/grasp.gif" width="230" height="165">
                </div>
                <img src='images/icra_nerfgrasp/grasp.png' width="230" height="165">
              </div>
              <script type="text/javascript">
                function icra_nerfgrasp_start() {
                  document.getElementById('icra_nerfgrasp_image').style.opacity = "1";
                }

                function icra_nerfgrasp_stop() {
                  document.getElementById('icra_nerfgrasp_image').style.opacity = "0";
                }
                icra_nerfgrasp_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://pku-epic.github.io/GraspNeRF/">
              <papertitle>GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent and Specular Objects Using Generalizable NeRF</papertitle>
              </a>
              <br>
              <a href="https://daiqy.github.io/index.html">Qiyu Dai*</a>,
              Yan Zhu*, Yiran Geng, Ciyu Ruan, <em><b style="font-weight: bold;">Jiazhao Zhang</b></em>, He Wang<sup>‚Ä†</sup>
              <br>
              <em><b style="font-weight: bold;">ICRA 2023</b></em>
              <br>
              <!-- <a href="https://dreamfusion3d.github.io/">project page</a> -->
              <!-- / -->
              <a href="https://arxiv.org/abs/2210.06575">Paper</a>
              /
              <a href="https://github.com/PKU-EPIC/GraspNeRF">Code & Data</a> 
              /
              <a href="https://pku-epic.github.io/GraspNeRF/">Project page</a>
              <p></p>
              <!-- <p>
                To realize efficient random optimization in the 18D state space of IMU tracking, we propose to identify and sample particles from active subspace.
              </p> -->
            </td>
          </tr>



          <tr onmouseout="aaai_hoi_stop()" onmouseover="aaai_hoi_start()" >
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='aaai_hoi_image'>
                <img src="images/aaai_hoi/HOTrack.gif" width="230" height="165">
                </div>
                <img src='images/aaai_hoi/HOTrack.png' width="230" height="165">
              </div>
              <script type="text/javascript">
                function aaai_hoi_start() {
                  document.getElementById('aaai_hoi_image').style.opacity = "1";
                }

                function aaai_hoi_stop() {
                  document.getElementById('aaai_hoi_image').style.opacity = "0";
                }
                aaai_hoi_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://pku-epic.github.io/HOtrack/">
              <papertitle>Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild</papertitle>
              </a>
              <br>
              <a href="https://jychen18.github.io/">Jiayi Chen*</a>,
              <a href="https://miyandoris.github.io/">Mi Yan*</a>,
              <em><b style="font-weight: bold;">Jiazhao Zhang</b></em>,
              <a href="https://xyz-99.github.io/">Yinzhen Xu</a>,
              <a href="https://dragonlong.github.io/">Xiaolong Li</a>,
              <a href="https://yijiaweng.github.io/">Yijiang Weng</a>,
              <a href="https://ericyi.github.io/">Li Yi</a>,
              <a href="https://www.cs.columbia.edu/~shurans/">Shuran Song</a>,
              <a href="https://hughw19.github.io/">He Wang<sup>‚Ä†</sup></a>
              <br>
              <em><b style="font-weight: bold;">AAAI 2023 (Oral Presentation)</b></em>
              <br>
              <!-- <a href="https://dreamfusion3d.github.io/">project page</a> -->
              <!-- / -->
              <a href="https://arxiv.org/abs/2209.12009">Paper</a>
              /
              <a href="https://github.com/PKU-EPIC/HOTrack">Code & Data</a>
              /
              <a href="https://pku-epic.github.io/HOtrack/">Project page</a>
              <p></p>
              <!-- <p>
                To realize efficient random optimization in the 18D state space of IMU tracking, we propose to identify and sample particles from active subspace.
              </p> -->
            </td>
          </tr>



          <tr onmouseout="asro_dio_stop()" onmouseover="asro_dio_start()" >
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='asro_dio_image'>
                <img src="images/asro_dio/asro_dio_video.gif" width="230" height="165">
                </div>
                <img src='images/asro_dio/asro_dio_image.png' width="230" height="165">
              </div>
              <script type="text/javascript">
                function asro_dio_start() {
                  document.getElementById('asro_dio_image').style.opacity = "1";
                }

                function asro_dio_stop() {
                  document.getElementById('asro_dio_image').style.opacity = "0";
                }
                asro_dio_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9915552">
              <papertitle>ASRO-DIO: Active Subspace Random Optimization Based Depth Inertial Odometry</papertitle>
              </a>
              <br>
              <em><b style="font-weight: bold;">Jiazhao Zhang</b></em>, Yijie Tang,
              <a href="https://hughw19.github.io/">He Wang</a>,
              <a href="http://kevinkaixu.net/index.html">Kai Xu<sup>‚Ä†</sup></a>
              <br>
              <em><b style="font-weight: bold;">Transactions on Robotics (T-RO 2022)</b></em>
              <br>
              <!-- <a href="https://dreamfusion3d.github.io/">project page</a> -->
              <!-- / -->
              <a href="https://ieeexplore.ieee.org/document/9915552">Paper</a>
              /
              <a href="zhngjizh@gmail.com">Contact me for code permission</a>  
              <p></p>
              <p>
                To realize efficient random optimization in the 18D state space of IMU tracking, we propose to identify and sample particles from active subspace.
              </p>
            </td>
          </tr>
		  
          <tr onmouseout="rosefusion_stop()" onmouseover="rosefusion_start()" >
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='rosefusion_image'>
                <img src="images/rosefusion/rosefusion_video.gif" width="230" height="165">
                </div>
                <img src='images/rosefusion/rosefusion_image.png' width="230" height="165">
              </div>
              <script type="text/javascript">
                function rosefusion_start() {
                  document.getElementById('rosefusion_image').style.opacity = "1";
                }

                function rosefusion_stop() {
                  document.getElementById('rosefusion_image').style.opacity = "0";
                }
                rosefusion_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2105.05600.pdf">
              <papertitle>ROSEFusion: Random Optimization for Online Dense Reconstruction under Fast Camera Motion</papertitle>
              </a>
              <br>
              <em><b style="font-weight: bold;">Jiazhao Zhang</b></em>,
              <a href="http://www.zhuchenyang.net/">Chenyang Zhu</a>,
              Lintao Zheng,
              <a href="http://kevinkaixu.net/index.html">Kai Xu<sup>‚Ä†</sup></a>
              <br>
              <em><b style="font-weight: bold;">ACM Transactions on Graphics (SIGGRAPH 2021)</b></em>
              <br>
              <!-- <a href="https://dreamfusion3d.github.io/">project page</a> -->
              <!-- / -->
              <a href="https://arxiv.org/pdf/2105.05600.pdf">Paper</a>
              /
              <a href="https://github.com/jzhzhang/ROSEFusion">Code & Data</a>
              <p></p>
              <p>
                We propose to tackle the difficulties of fast-motion camera tracking in the absence of inertial measurements using random optimization.
              </p>
            </td>
          </tr>

          <tr onmouseout="fconv_stop()" onmouseover="fconv_start()" >
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fconv_image'>
                <img src="images/fconv/fconv_video.gif" width="230" height="165">
                </div>
                <img src='images/fconv/fconv_image.png' width="230" height="165">
              </div>
              <script type="text/javascript">
                function fconv_start() {
                  document.getElementById('fconv_image').style.opacity = "1";
                }

                function fconv_stop() {
                  document.getElementById('fconv_image').style.opacity = "0";
                }
                fconv_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2003.06233">
              <papertitle>Fusion-Aware Point Convolution for Online Semantic 3D Scene Segmentation</papertitle>
              </a>
              <br>
              <em><b style="font-weight: bold;">Jiazhao Zhang</b></em>*,
              <a href="http://www.zhuchenyang.net/">Chenyang Zhu*</a>,
              Lintao Zheng,
              <a href="http://kevinkaixu.net/index.html">Kai Xu<sup>‚Ä†</sup></a>
              <br>
              <em><b style="font-weight: bold;">CVPR 2020</b></em>
              <br>
              <!-- <a href="https://dreamfusion3d.github.io/">project page</a> -->
              <!-- / -->
              <a href="https://arxiv.org/pdf/2003.06233">Paper</a>
              /
              <a href="https://github.com/jzhzhang/FusionAwareConv">Code & Data</a>
              <p></p>
              <p>
                We propose a novel fusionaware 3D point convolution which operates directly on the geometric surface being reconstructed and exploits effectively the inter-frame correlation for high quality 3D feature learning.              
              </p>
            </td>
          </tr>

          <tr onmouseout="asu_pg_stop()" onmouseover="asu_pg_start()" >
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='asu_pg_image'>
                <img src="images/asu_pg/asu_pg_image.jpg" width="230" height="165">
                </div>
                <img src='images/asu_pg/asu_pg_image.jpg' width="230" height="165">
              </div>
              <script type="text/javascript">
                function asu_pg_start() {
                  document.getElementById('asu_pg_image').style.opacity = "1";
                }

                function asu_pg_stop() {
                  document.getElementById('asu_pg_image').style.opacity = "0";
                }
                asu_pg_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1906.07409">
              <papertitle>Active Scene Understanding via Online Semantic Reconstruction</papertitle>
              </a>
              <br>
              Lintao Zheng, Chenyang Zhu, <em><b style="font-weight: bold;">Jiazhao Zhang</b></em>, Hang Zhao, Hui Huang, Matthias Niessner, Kai Xu<sup>‚Ä†</sup>
              <br>
              <em><b style="font-weight: bold;">Computer Graphics Forum (Pacific Graphics 2019)</b></em>
              <br>
              <!-- <a href="https://dreamfusion3d.github.io/">project page</a> -->
              <!-- / -->
              <a href="https://arxiv.org/pdf/1906.07409">Paper</a>
              <p></p>
              <p>
                We propose a novel approach to robot-operated active understanding of unknown indoor scenes, based on online RGBD reconstruction with semantic segmentation.            
              </p>
            </td>
          </tr>



    

        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td> -->

            <td width="75%" valign="center">
              Conference Reviewer: RSS, ICCV, NeurIPS, CVPR, ICLR, ICRA, IROS <br>
              Journal Reviewer: TPAMI, TIP, RA-L
            </td>
          </tr>

				
        </tbody></table>
        


				
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
              Template adapted from <a href="https://jonbarron.info/">Jon Barron</a>.
              <br> Last updated: April 2023
              </p>
            </td>
          </tr>
        </tbody></table>



      </td>
    </tr>
  </table>
</body>

</html>
